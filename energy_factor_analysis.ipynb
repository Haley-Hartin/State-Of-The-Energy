{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "#### This section of the notebooks reads in the data files and stores them im pandas dataframes.\n",
    "The dataframes frames in this section all have colums of [1960 - 2019] and rows for each state execpt for industy_gdp_by_state_df which goes from [1997-2020] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(os.getcwd(), \"data/csv\")\n",
    "excel_path = os.path.join(os.getcwd(), \"data/excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in all datasets here \n",
    "\n",
    "vehicle_registration_df = pd.read_csv(os.path.join(csv_path, \"vehicle_registrations_by_state.csv\"))\n",
    "energy_consumption_per_real_gdp_df = pd.read_csv(os.path.join(csv_path, \"energy_consumption_per_real_gdp.csv\"))\n",
    "current_dollar_gdp_df = pd.read_csv(os.path.join(csv_path, \"Current_dollar_GDP.csv\")) #in millions\n",
    "total_consuption_df = pd.read_csv(os.path.join(csv_path, \"total_consuption.csv\")) #in billion Btu\n",
    "industy_gdp_by_state_df = pd.read_csv(os.path.join(csv_path, \"industy_gdp_by_state.csv\"))\n",
    "total_population_df = pd.read_csv(os.path.join(csv_path, \"total_population.csv\"))\n",
    "real_gdp_df = pd.read_csv(os.path.join(csv_path, \"real_GDP.csv\")) #in millions\n",
    "ghg_emissions_df = pd.read_csv(os.path.join(csv_path, \"GHG_Emissions.csv\"))\n",
    "co2_emissions_df = pd.read_excel(os.path.join(excel_path, \"co2_emissions.xlsx\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "#### This section of the notebook cleans the data frames. \n",
    "\n",
    "We start by dropping all the unneeded columns so that each data frame has the same colums. Then we drop any columns that have missing values. This only leaves us with a few columns, so it might be a better idea in the future to replace empty values with the mean value for that year or something similar. \n",
    "\n",
    "This section is only evaluting vehicle registration, population and GDP against energy consuption as a proof of concept. We will need to clean and add all the other dataframes to this model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_names=[\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_abbreviations = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_columns_to_drop = ['Unnamed: 91', 'Unnamed: 92', 'Unnamed: 93', 'Unnamed: 94',\n",
    "       'Unnamed: 95', 'Unnamed: 96', 'Unnamed: 97', 'Unnamed: 98',\n",
    "       'Unnamed: 99', 'Unnamed: 100','Unnamed: 62', 'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65',\n",
    "       'Unnamed: 66', 'Unnamed: 67', 'Unnamed: 68', 'Unnamed: 69',\n",
    "       'Unnamed: 70', 'Unnamed: 71', 'Unnamed: 72', 'Unnamed: 73',\n",
    "       'Unnamed: 74', 'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77',\n",
    "       'Unnamed: 78', 'Unnamed: 79', 'Unnamed: 80', 'Unnamed: 81',\n",
    "       'Unnamed: 82', 'Unnamed: 83', 'Unnamed: 84', 'Unnamed: 85',\n",
    "       'Unnamed: 86', 'Unnamed: 87', 'Unnamed: 88', 'Unnamed: 89',\n",
    "       'Unnamed: 90', 'Unnamed: 61']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    srings_to_replace = [\"(NA)\", \"(L)\", \"(D)\", \"Change\", \"(2000–2018)\", \"Percent\", \"Absolute\"]\n",
    "    unnamed_to_drop = list(set(df.columns).intersection(unnamed_columns_to_drop))\n",
    "    df = df.drop(columns = unnamed_to_drop)\n",
    "    \n",
    "    null_values_allowed_before_column_is_dropped = 40\n",
    "    columns_to_drop = []\n",
    "    first_col = df.columns[0]\n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index, first_col] = str(df.at[index, first_col]).replace(\"(Items)\", \"\").strip()\n",
    "\n",
    "    for col in df.columns[1:]:\n",
    "        df.loc[df[col].isin(srings_to_replace), col] = np.nan\n",
    "        df[col] = df[col].astype(float)\n",
    "        \n",
    "        if(df[col].isna().sum() > null_values_allowed_before_column_is_dropped):\n",
    "            columns_to_drop.append(col)\n",
    "        else:\n",
    "            df[col].fillna(value=pd.to_numeric(df[col], errors='coerce').mean(), inplace=True)\n",
    "    df = df.drop( columns = columns_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_registration_df.drop(index = [0,1,2,3,4,5,6,7,8,9,61,62,64,63], inplace = True)\n",
    "vehicle_registration_df = clean_dataframe(vehicle_registration_df)\n",
    "vehicle_registration_df =  vehicle_registration_df[vehicle_registration_df['Years'].isin(state_names)]\n",
    "vehicle_registration_columns = vehicle_registration_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_population_df = clean_dataframe(total_population_df)\n",
    "total_population_df =  total_population_df[total_population_df['State'].isin(state_abbreviations)]\n",
    "total_population_columns = total_population_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consuption_df = clean_dataframe(total_consuption_df)\n",
    "total_consuption_df =  total_consuption_df[total_consuption_df['State'].isin(state_abbreviations)]\n",
    "total_consuption_df_columns = total_consuption_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_gdp_df = clean_dataframe(real_gdp_df)\n",
    "real_gdp_df =  real_gdp_df[real_gdp_df['State'].isin(state_abbreviations)]\n",
    "real_gdp_df_columns = real_gdp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "industy_gdp_by_state_df = industy_gdp_by_state_df.drop(columns = ['GeoFIPS','Region','TableName','LineCode','IndustryClassification','Description','Unit'])\n",
    "industy_gdp_by_state_df = clean_dataframe(industy_gdp_by_state_df)\n",
    "industy_gdp_by_state_df = industy_gdp_by_state_df.groupby('GeoName').mean().reset_index()\n",
    "industy_gdp_by_state_df =  industy_gdp_by_state_df[industy_gdp_by_state_df['GeoName'].isin(state_names)]\n",
    "industy_gdp_by_state_df_columns = industy_gdp_by_state_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_emissions_df = co2_emissions_df.rename({'Table 2. State energy-related carbon dioxide emissions by year, adjusted (2000–2018)': 'State', 'Unnamed: 1': '2000', 'Unnamed: 2': '2001', 'Unnamed: 3': '2002', 'Unnamed: 4': '2003', 'Unnamed: 5': '2004',\n",
    "       'Unnamed: 6': '2005', 'Unnamed: 7': '2006', 'Unnamed: 8': '2007', 'Unnamed: 9': '2008', 'Unnamed: 10': '2009',\n",
    "       'Unnamed: 11': '2010', 'Unnamed: 12': '2011', 'Unnamed: 13': '2012', 'Unnamed: 14': '2013',\n",
    "       'Unnamed: 15': '2014', 'Unnamed: 16': '2015', 'Unnamed: 17': '2016', 'Unnamed: 18': '2017',\n",
    "       'Unnamed: 19': '2018', 'Unnamed: 20': '2019', 'Unnamed: 21': '2020'}, axis =1).reset_index()\n",
    "co2_emissions_df = co2_emissions_df.drop(columns = ['index'])\n",
    "co2_emissions_df =  co2_emissions_df[co2_emissions_df['State'].isin(state_names)]\n",
    "co2_emissions_df_columns = co2_emissions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2013',\n",
       " '2009',\n",
       " '2018',\n",
       " '2014',\n",
       " '2010',\n",
       " '2012',\n",
       " '2017',\n",
       " '2011',\n",
       " '2016',\n",
       " '2019',\n",
       " '2007',\n",
       " '2015',\n",
       " '2008']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the columns that are in each dataframe after columns with empty values have been dropped. \n",
    "columns_to_evaluate = list(set(vehicle_registration_columns).intersection(total_population_columns).intersection(total_consuption_df_columns).intersection(real_gdp_df_columns).intersection(industy_gdp_by_state_df_columns).intersection(co2_emissions_df_columns))\n",
    "columns_to_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure each column we are going to evaluate has the same number of values \n",
    "for col in columns_to_evaluate:\n",
    "    if(not (len(vehicle_registration_df[col]) == len(total_consuption_df[col]) == len(total_population_df[col]) == len(real_gdp_df[col])== len(industy_gdp_by_state_df[col]) == len(co2_emissions_df[col]))):\n",
    "        print(\"unequal entries for column:\" + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the data frames and add each value to data_point_pairs array. \n",
    "# The data_point_pairs array will be the [vehicle registration, population, GDP, Industry GDP] value for each year and each state\n",
    "# The total_consumption_vals will be the cooresponding energy consuption value \n",
    "# for the [vehicle registration, population, GDP, Industry GDP, C02 emissions] data point\n",
    "data_point_pairs = []\n",
    "total_consumption_vals = []\n",
    "for col in columns_to_evaluate:\n",
    "    for i in range(0,50):\n",
    "        pair = [vehicle_registration_df.iloc[i][col], total_population_df.iloc[i][col], real_gdp_df.iloc[i][col], industy_gdp_by_state_df.iloc[i][col], co2_emissions_df.iloc[i][col]]\n",
    "        data_point_pairs.append(pair)\n",
    "        \n",
    "        total_consumption_vals.append(total_consuption_df.iloc[i][col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle registration: 4787219.0\n",
      "population:  738.0\n",
      "GDP:  54748.0\n",
      "Industry GDP:  11241.679347826086\n",
      "C02 emissions:  121.1630059889289\n",
      "total energy consuption: 597975.0\n"
     ]
    }
   ],
   "source": [
    "print(\"vehicle registration:\" , data_point_pairs[0][0])\n",
    "print(\"population: \", data_point_pairs[0][1])\n",
    "print(\"GDP: \", data_point_pairs[0][2])\n",
    "print(\"Industry GDP: \", data_point_pairs[0][3])\n",
    "print(\"C02 emissions: \", data_point_pairs[0][4])\n",
    "print(\"total energy consuption:\" ,total_consumption_vals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "#### This section of the notebooks creates a linear regression moel for energy consuption.\n",
    "\n",
    "We will need to add the climate data.\n",
    "\n",
    "In the model summary, x1 represents vehicle regisration, x2 represents population and x3 represents GDP, x4 represents Industry GDP, x5 represents C02 emissions. There are some other values in the summary that give us a good indication as to how well our model fits energy consuption such at the r squared value and F statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted energy consumpion for \n",
      "vehicle registration = 4610845 , population = 699 (10,000), GDP = 55911, Industry GDP = 9717, C02 emissions = 121 \n",
      " [1020226.77747812]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.931</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.931</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   1742.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 16 Nov 2021</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:29:00</td>     <th>  Log-Likelihood:    </th>          <td> -9713.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   650</td>      <th>  AIC:               </th>          <td>1.944e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   645</td>      <th>  BIC:               </th>          <td>1.946e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>   -0.0567</td> <td>    0.014</td> <td>   -4.155</td> <td> 0.000</td> <td>   -0.084</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>  207.2993</td> <td>   24.518</td> <td>    8.455</td> <td> 0.000</td> <td>  159.154</td> <td>  255.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    0.3735</td> <td>    0.399</td> <td>    0.936</td> <td> 0.350</td> <td>   -0.410</td> <td>    1.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>   -8.4666</td> <td>    2.556</td> <td>   -3.313</td> <td> 0.001</td> <td>  -13.485</td> <td>   -3.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td> 9510.9410</td> <td>  444.563</td> <td>   21.394</td> <td> 0.000</td> <td> 8637.975</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>628.378</td> <th>  Durbin-Watson:     </th> <td>   1.764</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>58242.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 3.983</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>48.684</td>  <th>  Cond. No.          </th> <td>1.13e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.13e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.931\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.931\n",
       "Method:                 Least Squares   F-statistic:                              1742.\n",
       "Date:                Tue, 16 Nov 2021   Prob (F-statistic):                        0.00\n",
       "Time:                        10:29:00   Log-Likelihood:                         -9713.5\n",
       "No. Observations:                 650   AIC:                                  1.944e+04\n",
       "Df Residuals:                     645   BIC:                                  1.946e+04\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.0567      0.014     -4.155      0.000      -0.084      -0.030\n",
       "x2           207.2993     24.518      8.455      0.000     159.154     255.444\n",
       "x3             0.3735      0.399      0.936      0.350      -0.410       1.157\n",
       "x4            -8.4666      2.556     -3.313      0.001     -13.485      -3.448\n",
       "x5          9510.9410    444.563     21.394      0.000    8637.975    1.04e+04\n",
       "==============================================================================\n",
       "Omnibus:                      628.378   Durbin-Watson:                   1.764\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            58242.750\n",
       "Skew:                           3.983   Prob(JB):                         0.00\n",
       "Kurtosis:                      48.684   Cond. No.                     1.13e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.13e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A potential library we can use for regression analysis \n",
    "# normalize population\n",
    "\n",
    "X = data_point_pairs\n",
    "y = total_consumption_vals\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X,y)\n",
    "\n",
    "#predict energy consuption for vehicle registration = 4610845 , population =699 (10,000), GDP = 55911, Industry GDP = 9717, C02 emissions = 121\n",
    "predictions = lm.predict([[4610845, 699, 55911, 9717, 121]])\n",
    "print(\"Predicted energy consumpion for \\nvehicle registration = 4610845 , population = 699 (10,000), GDP = 55911, Industry GDP = 9717, C02 emissions = 121 \\n\", predictions )\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
